# -*- coding: utf-8 -*-
"""DDPS_Scraping_BT.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zKpzYFTl98X2AZSwHiM_w-EdOkH-BVzb

# Bundestag Scraping
## Preperation: Aquire Raw Data
"""

# Import modules
from bs4 import BeautifulSoup
# folder od_lib has to be in cwd
#import od_lib.definitions.path_definitions as path_definitions
import requests
import os
import regex
import time
import pandas as pd
import numpy as np
import re

from google.colab import drive
drive.mount('/content/drive')

"""As the REST-API of the Bundestag did not allow for full-text or keyword search, all plenary speeches of the 20th election period of the Bundestag had to be scrapped. As these are just available as .pdf or .xml, we built on the "open-discourse" project to extract the data:
https://github.com/open-discourse/open-discourse
"""

## Download Raw Data

# Set output directory
ELECTORAL_TERM_19_20_OUTPUT = path_definitions.ELECTORAL_TERM_19_20_STAGE_01

if not os.path.exists(ELECTORAL_TERM_19_20_OUTPUT):
    os.makedirs(ELECTORAL_TERM_19_20_OUTPUT)

election_periods = [
    {
        "election_period": 20,
        "url": "https://www.bundestag.de/ajax/filterlist/de/services/opendata/866354-866354?offset={}",  # noqa
    },
]

for election_period in election_periods:
    OUTPUT_PATH = os.path.join(
        ELECTORAL_TERM_19_20_OUTPUT,
        "electoral_term_{}".format(election_period["election_period"]),
    )

    if not os.path.exists(OUTPUT_PATH):
        os.makedirs(OUTPUT_PATH)

    reached_end = False
    offset = 0

    while not reached_end:
        URL = election_period["url"].format(str(offset))
        page = requests.get(URL, headers={"User-Agent": "Mozilla/5.0"})

        soup = BeautifulSoup(page.text, "html.parser")
        reached_end = True

        # scrape for links
        for link in soup.find_all("a", attrs={"href": regex.compile("xml$")}):
            reached_end = False
            url = "https://www.bundestag.de" + link.get("href")
            page = requests.get(url, headers={"User-Agent": "Mozilla/5.0"})
            session = regex.search(r"\d{5}(?=-data\.xml)", url).group(0)

            print(session)
            with open(os.path.join(OUTPUT_PATH, session + ".xml"), "w") as file:
                file.write(
                    regex.sub(
                        "</sub>",
                        "",
                        regex.sub("<sub>", "", page.content.decode("utf-8")),
                    )
                )

            time.sleep(0.1)
        offset += 10

# Split the Data

import od_lib.definitions.path_definitions as path_definitions
import xml.etree.ElementTree as et

# input directory
ELECTORAL_TERM_19_20_INPUT = path_definitions.ELECTORAL_TERM_19_20_STAGE_01

# output directory
ELECTORAL_TERM_19_20_OUTPUT = path_definitions.ELECTORAL_TERM_19_20_STAGE_02

for electoral_term_folder in sorted(os.listdir(ELECTORAL_TERM_19_20_INPUT)):
    electoral_term_folder_path = os.path.join(
        ELECTORAL_TERM_19_20_INPUT, electoral_term_folder
    )

    # Skip e.g. the .DS_Store file.
    if not os.path.isdir(electoral_term_folder_path):
        continue

    for xml_file in sorted(os.listdir(electoral_term_folder_path)):

        print(xml_file)

        save_path = os.path.join(
            ELECTORAL_TERM_19_20_OUTPUT,
            electoral_term_folder,
            regex.search(r"\d+", xml_file).group(),
        )

        # Read in the data
        tree = et.parse(os.path.join(electoral_term_folder_path, xml_file))
        root = tree.getroot()

        toc = et.ElementTree(root.find("vorspann"))
        session_content = et.ElementTree(root.find("sitzungsverlauf"))
        appendix = et.ElementTree(root.find("anlagen"))
        meta_data = et.ElementTree(root.find("rednerliste"))

        if not os.path.exists(save_path):
            os.makedirs(save_path)

        # Save single xmls
        toc.write(
            os.path.join(save_path, "toc.xml"), encoding="UTF-8", xml_declaration=True
        )
        session_content.write(
            os.path.join(save_path, "session_content.xml"),
            encoding="UTF-8",
            xml_declaration=True,
        )
        appendix.write(
            os.path.join(save_path, "appendix.xml"),
            encoding="UTF-8",
            xml_declaration=True,
        )
        meta_data.write(
            os.path.join(save_path, "meta_data.xml"),
            encoding="UTF-8",
            xml_declaration=True,
        )

# Export relevant information to a datafrane

from od_lib.helper_functions.extract_contributions import extract
import od_lib.definitions.path_definitions as path_definitions
import pandas as pd
import numpy as np
import xml.etree.ElementTree as et
import regex
import os
import datetime


# input directory
ELECTORAL_TERM_19_20_INPUT = path_definitions.ELECTORAL_TERM_19_20_STAGE_02

# output directory
ELECTORAL_TERM_19_20_OUTPUT = path_definitions.ELECTORAL_TERM_19_20_STAGE_03
CONTRIBUTIONS_SIMPLIFIED = path_definitions.CONTRIBUTIONS_SIMPLIFIED

if not os.path.exists(ELECTORAL_TERM_19_20_OUTPUT):
    os.makedirs(ELECTORAL_TERM_19_20_OUTPUT)


if not os.path.exists(CONTRIBUTIONS_SIMPLIFIED):
    os.makedirs(CONTRIBUTIONS_SIMPLIFIED)

# Classify Parties 
faction_patterns = {
    "Bündnis 90/Die Grünen": r"(?:BÜNDNIS\s*(?:90)?/?(?:\s*D[1I]E)?|Bündnis\s*90/(?:\s*D[1I]E)?)?\s*[GC]R[UÜ].?\s*[ÑN]EN?(?:/Bündnis 90)?",  # noqa: E501
    "CDU/CSU": r"(?:Gast|-)?(?:\s*C\s*[DSMU]\s*S?[DU]\s*(?:\s*[/,':!.-]?)*\s*(?:\s*C+\s*[DSs]?\s*[UÙ]?\s*)?)(?:-?Hosp\.|-Gast|1)?",  # noqa: E501
    "BP": r"^BP",
    "DA": r"^DA",
    "DP": r"^DP",
    "DIE LINKE.": r"DIE LINKE",
    "DPB": r"^DPB",
    "DRP": r"DRP(\-Hosp\.)?|^SRP|^DBP",
    "FDP": r"\s*F\.?\s*[PDO][.']?[DP]\.?",
    "Fraktionslos": r"(?:fraktionslos|Parteilos)",
    "FU": r"^FU",
    "FVP": r"^FVP",
    "Gast": r"Gast",
    "GB/BHE": r"(?:GB[/-]\s*)?BHE(?:-DG)?",
    "KPD": r"^KPD",
    "NR": r"^NR$",
    "PDS": r"(?:Gruppe\s*der\s*)?PDS(?:/(?:LL|Linke Liste))?",
    "SPD": r"\s*'?S(?:PD|DP)(?:\.|-Gast)?",
    "SSW": r"^SSW",
    "SRP": r"^SRP",
    "WAV": r"^WAV",
    "Z": r"^Z$",
    "AfD": r"^AfD$",
    "DBP": r"^DBP$",
}




class Incrementor(object):
    """Incrementor class for iterative regex deletion"""

    def __init__(self):
        self.count = -1

    def increment(self, matchObject):
        self.count += 1
        return "{->" + str(self.count) + "}"


def get_first_last(name):
    first_last = name.split()
    if len(first_last) == 1:
        first_name = ""
        last_name = first_last[0]
    elif len(first_last) >= 2:
        first_name = first_last[:-1]
        last_name = first_last[-1]
    else:
        first_name = "ERROR"
        last_name = "ERROR"
    return " ".join(first_name), last_name


def get_faction_abbrev(faction, faction_patterns):
    """matches the given faction and returns an id"""

    for faction_abbrev, faction_pattern in faction_patterns.items():
        if regex.search(faction_pattern, faction):
            return faction_abbrev
    return None


speech_content_id = 1000000

speech_content = pd.DataFrame(
    {
        "id": [],
        "session": [],
        "first_name": [],
        "last_name": [],
        "faction_id": [],
        "position_short": [],
        "position_long": [],
        "politician_id": [],
        "speech_content": [],
        "date": [],
    }
)




for electoral_term_folder in sorted(os.listdir(ELECTORAL_TERM_19_20_INPUT)):
    electoral_term_folder_path = os.path.join(
        ELECTORAL_TERM_19_20_INPUT, electoral_term_folder
    )
    if not os.path.isdir(electoral_term_folder_path):
        continue
    CONTRIBUTIONS_EXTENDED_OUTPUT = os.path.join(
        path_definitions.CONTRIBUTIONS_EXTENDED_STAGE_01, electoral_term_folder
    )
    ELECTORAL_TERM_SPOKEN_CONTENT = os.path.join(
        ELECTORAL_TERM_19_20_OUTPUT, electoral_term_folder, "speech_content"
    )
    CONTRIBUTIONS_SIMPLIFIED_OUTPUT = os.path.join(
        CONTRIBUTIONS_SIMPLIFIED, electoral_term_folder
    )
    if not os.path.exists(CONTRIBUTIONS_EXTENDED_OUTPUT):
        os.makedirs(CONTRIBUTIONS_EXTENDED_OUTPUT)

    if not os.path.exists(ELECTORAL_TERM_SPOKEN_CONTENT):
        os.makedirs(ELECTORAL_TERM_SPOKEN_CONTENT)

    if not os.path.exists(CONTRIBUTIONS_SIMPLIFIED_OUTPUT):
        os.makedirs(CONTRIBUTIONS_SIMPLIFIED_OUTPUT)

    speech_records = []

    contributions_simplified = pd.DataFrame(
        {"text_position": [], "content": [], "speech_id": []}
    )
    for session in sorted(os.listdir(electoral_term_folder_path)):
        if session == ".DS_Store":
            continue

        contributions_extended = pd.DataFrame(
            {
                "id": [],
                "type": [],
                "name": [],
                "faction": [],
                "constituency": [],
                "content": [],
                "text_position": [],
            }
        )

        print(session)

        session_content = et.parse(
            os.path.join(electoral_term_folder_path, session, "session_content.xml")
        )
        meta_data = et.parse(
            os.path.join(electoral_term_folder_path, session, "meta_data.xml")
        )
        date = meta_data.getroot().get("sitzung-datum")
        
        # Wrong date in xml file. Fixing manually
        if session == "19158":
            date = "07.05.2020"
        date = (
            datetime.datetime.strptime(date, "%d.%m.%Y") - datetime.datetime(1970, 1, 1)
        ).total_seconds()

        root = session_content.getroot()

        tops = root.findall("tagesordnungspunkt")

        c = Incrementor()
        id_Counter = 0

        for top in tops:
            speeches = top.findall("rede")
            for speech in speeches:
                speaker = speech[0].find("redner")
                try:
                    speaker_id = int(speaker.get("id"))
                except (ValueError, AttributeError):
                    speaker_id = -1

                try:
                    name = speaker.find("name")
                except AttributeError:
                    continue

                try:
                    first_name = name.find("vorname").text
                except AttributeError:
                    first_name = ""

                try:
                    last_name = name.find("nachname").text
                except AttributeError:
                    last_name = ""

                try:
                    position_raw = name.find("fraktion").text
                except (ValueError, AttributeError):
                    position_raw = name.find("rolle").find("rolle_lang").text
                faction_abbrev = get_faction_abbrev(
                    str(position_raw), faction_patterns=faction_patterns
                )

                speech_text = ""
                text_position = 0
                for content in speech[1:]:
                    tag = content.tag
                    if tag == "name":
                        speech_records.append(
                            {
                                "id": speech_content_id,
                                "session": session,
                                "first_name": first_name,
                                "last_name": last_name,
                                "speech_content": speech_text,
                                "date": date,
                            }
                        )
                        speech_content_id += 1
                        faction_id = -1
                        speaker_id = -1
                        name = regex.sub(":", "", content.text).split()
                        first_name, last_name = get_first_last(" ".join(name[1:]))

                        speech_text = ""
                        text_position = 0
                    elif tag == "p" and content.get("klasse") == "redner":
                        speech_records.append(
                            {
                                "id": speech_content_id,
                                "session": session,
                                "first_name": first_name,
                                "last_name": last_name,
                                "speech_content": speech_text,
                                "date": date,
                            }
                        )

                        speech_content_id += 1
                        speech_text = ""
                        text_position = 0
                    elif tag == "p":
                        try:
                            speech_text += "\n\n" + content.text
                        except TypeError:
                            pass
                    elif tag == "kommentar":
                        (
                            contributions_extended_frame,
                            speech_replaced,
                            contributions_simplified_frame,
                            text_position,
                        ) = extract(
                            content.text,
                            int(session),
                            speech_content_id,
                            text_position,
                            False,
                        )
                        speech_text += "\n\n" + speech_replaced
                        contributions_extended = pd.concat(
                            [contributions_extended, contributions_extended_frame],
                            sort=False,
                        )
                        contributions_simplified = pd.concat(
                            [contributions_simplified, contributions_simplified_frame],
                            sort=False,
                        )

                speech_records.append(
                    {
                        "id": speech_content_id,
                        "session": session,
                        "first_name": first_name,
                        "last_name": last_name,
                        "date": date,
                    }
                )
                speech_content_id += 1

        contributions_extended.to_pickle(
            os.path.join(CONTRIBUTIONS_EXTENDED_OUTPUT, session + ".pkl")
        )

    speech_content = pd.DataFrame.from_records(speech_records)

    speech_content.to_pickle(
        os.path.join(ELECTORAL_TERM_SPOKEN_CONTENT, "speech_content.pkl")
    )

    contributions_simplified.to_pickle(
        os.path.join(
            CONTRIBUTIONS_SIMPLIFIED_OUTPUT,
            "contributions_simplified.pkl",
        )
    )

# Extract needed speeches, adjust time & export raw data
Speeches = speech_content
Speeches.date = pd.to_datetime(speech_content.date, unit="s")
Speeches.to_excel("Speeches_BT.xlsx")

"""## Data Transformation and Counting of Keywords"""

# Reimport Data after Restart
Speeches = pd.read_excel("/content/drive/MyDrive/Speeches_BT.xlsx")

# Control for distribution of dates
import matplotlib.pyplot as plt
import seaborn as sns

Speeches.date.unique().shape
plt.figure(figsize=(15, 8))
sns.histplot(Speeches.date, bins=100)

# Clean Speeches: delete paragraphs, punctuation and convert to lower case
df = Speeches
df["speech_content"].replace('\n', " ",regex=True, inplace=True)
df["speech_content"] = df["speech_content"].str.lower()
df["speech_content"] = df["speech_content"].astype(str).apply(lambda x: re.sub('[^a-zA-ZäöüÄÖÜß]', ' ', x))
df

# Group all speeches by date
summary_df = df.groupby('date')['speech_content'].apply(lambda x: ' '.join(x)).reset_index()
summary_df

"""### Import keywords & Create Counting-Matrix"""

# Defining query dict
keywords_bt = {
    "arms_general": ["waffenlieferung", "waffen lieferungen", "waffen lieferung", "waffenlieferungen", "waffen", "lieferungen"],
    "mech_tanks": ["panzerlieferungen", "panzerlieferung", "marder schützenpanzer", "leopard1", "panzer", "leopard"],
    "aa_general": ["flugabwehrrakete", "flugabwehr", "flugabwehr rakete", "flugabwehrsystem", "flugabwehrsysteme", "manpad", "manpads", "luftabwehr", "luftabwehrgeräte", "luft abwehr", "luftabwehr", "luftabwehrsysteme", "flug abwehrsystem", "flug abwehr"],
    "heavy_arms": ["schwere waffen", "offensivwaffen", "offensiv"],
    "at_general": ["panzerabwehrwaffen", "panzer abwehrwaffen"],
    "helmets": ["helme", "helm"],
    "at_panzerfaust": ["panzerfäuste", "panzer fäuste", "panzerfaust", "faust", "pzf", "pzf3"],
    "aa_stinger": ["stringerraketen", "stinger"],
    "aa_strela": ["strelaraketen", "strela"],
    "aa_gepard": ["gepard", "flakpanzer", "flakpz", "flugabwehrkanonenpanzer", "flak panzer", "defensivwaffe", "defensiv waffe"],
    "mech_pzh": ["panzerhaubitzen", "panzerhaubitze", "haubitze", "haubitzen", "pzh2000", "pzh", "pzh 2000"],
    "mech_ringtausch": ["ringtausch", "t72", "radpanzer", "schützenpanzer", "schützen panzer", "marder", "fuchs", "slovenien", "slovenisch", "slovenische", "slovenischer", "slovenisches", "polen", "polnische", "polnisch", "polnischer", "polnisches", "tschechien", "tschechisch", "tschechische", "tschechischer", "tschechisches", "tschechen", "griechenland", "griechen", "griechisch", "griechische", "griechischer", "bmp", "bmp1"],
    "at_iris": ["iris", "irist", "iristslm", "tslm", "patriot", "flugabwehrsystem"],
    "mech_mlrs": ["raketenwerfer", "mehrfachraketenwerfer", "mars", "raketen werfer"],
    
    "sum" : ["waffenlieferung","waffenlieferungen","waffenlieferung","waffenlieferungen","waffen","flugabwehrrakete","flugabwehr","flugabwehrrakete","flugabwehrsystem","flugabwehrsysteme","manpad","manpads","luftabwehr","luftabwehrgeräte","luftabwehr","luftabwehr","luftabwehrsysteme","flugabwehrsystem","flugabwehr","schwerewaffen","offensivwaffen","offensiv","panzerabwehrwaffen","panzerabwehrwaffen","helme","helm","panzerfäuste","panzerfäuste","panzerfaust","faust","pzf","pzf3","stringerraketen","stinger","strelaraketen","strela","gepard","flakpanzer","flakpz","flugabwehrkanonenpanzer","flakpanzer","defensivwaffe","defensivwaffe","panzerhaubitzen","panzerhaubitze","haubitze","haubitzen","pzh2000","pzh","pzh2000","ringtausch","t72","radpanzer","schützenpanzer","schützenpanzer","marder","fuchs","slovenien","slovenisch","slovenische","slovenischer","slovenisches","polen","polnische","polnisch","polnischer","polnisches","tschechien","tschechisch","tschechische","tschechischer","tschechisches","tschechen","griechenland","griechen","griechisch","griechische","griechischer","bmp","bmp1","iris","irist","iristslm","tslm","patriot","flugabwehrsystem","raketenwerfer","mehrfachraketenwerfer","mars","raketenwerfer","panzerlieferungen","panzerlieferung","marderschützenpanzer","leopard1","panzer","leopard"]
}

# Create a matrix of number of appearances of the keywords
df = summary_df
df['keywords_found'] = df['speech_content'].apply(lambda x: [word for word in x.split() if word in [word for sublist in keywords_bt.values() for word in sublist]])

queries = [*keywords_bt]
result_df = pd.DataFrame({'date': df['date']})
for query in queries:
    result_df[query] = 0
result_df["keywords_found"] = df['speech_content'].str.lower().str.findall(r'\w+')

# Create word count per row
df["wc"] = df['speech_content'].str.split().str.len()

# for each row in the new dataframe, count the frequency of each keyword in the 'keywords_found' list and fill the corresponding columns with the counts
for i, row in result_df.iterrows():
    for query, keywords in keywords_bt.items():
        regex_pattern = '|'.join([r"\b{}\b(?!\w)".format(kw.lower()) for kw in keywords]) 
        # divide by word count
        count = sum([1 for word in row['keywords_found'] if re.search(regex_pattern, word)])/df["wc"][i]
        result_df.at[i, query] = count


# Filter for speeches >01/01/2022 & drop unnecessary columns
final_bt = result_df.loc[(result_df['date'] > '2022-1-1')].drop("keywords_found", axis=1)
final_bt

# Visual control
import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(15, 5))
plt.title("Count of all keywords, BT")
sns.lineplot(data=final_bt, x="date", y="sum");

# Export data set
final_bt.to_excel("/content/drive/MyDrive/BT_final.xlsx")